\documentclass[12pt, twoside]{report}  
\usepackage[toc,page]{appendix}
\usepackage[english]{minitoc}


\begin{document}
\dominitoc
\tableofcontents								     

\chapter{Intro}

\textbf{Objectif :} c'est plus ici qu'il serait intéressant au début de définir le champ de l'environmental science.

\chapter{Chapitre problématique et données}



\section{Présentation de l'Agence}

\textbf{Objectifs :} Introduire le financeur de la thèse, c'est lui qui a déterminé les objectifs de la thèse donc cette section donne plus de contexte.  

\subsection{Objectifs}

\textbf{Objectifs :} Parler de la santé publique environnementale dont une des missions est la surveillance des pesticides (mais pas que).  

\subsection{Moyens}

\textbf{Objectifs :} Parler du réseau partenaire, centralisation de beaucoup de données d'autres instituts/labos mais n'est pas propriétaire des données.   

\subsection{Différences avec d'autres pays}

\textbf{Objectifs :} trouver des références, je sais que ça vient d'un des rapporteurs du papier qui nous a dit que ça se passait pas comme ça dans les autres pays, mais faut que je trouve une preuve de ça dans une référence. \\

\textbf{Transition :} La mission qui nous intéresse porte sur les pesticides.

\section{La mission de phytopharmacovigilance et ses spécificités}

\textbf{Objectif :} Pourquoi on regarde ça et essayer de préparer le terrain pour 2.3 et 2.4.. Expliquer que lorsqu'on utilise un produit actif ses mécanismes d'action sur sa cible peuvent également agir sur d'autres organismes (qui ne sont pas la cible). L'épandage se fait dans une zone (prévention ou traitement d'une culture cible) et du fait de son introduction dans l'environnement peut diffuser. On peut alors observer des contaminations humaines ou autres (végétales animales). (s'appuyer sur la fiche description ppv site anses sans la partie réseau partenaire qu'on garde en 2.3 et 2.4). Noter que la diffusion est influencée par la structure géographique (donner à titre d'exemple les HER) où des conditions météos.     

%\textbf{Objectifs :} Description du parcours d'une molécule de son épandage à sa rentrée en contact potentielle avec des organismes vivants. 

%\subsection{Epandage d'une substance}

%\textbf{Objectifs :} Description des différentes méthodes d'épandage, spécifique aux types de cultures. 

%\subsection{Propagation dans un milieu environnemental}

%\textbf{Objectifs :} Dispersion d'une substance épandue dans les milieux environnementaux, décrire les compartiments environnementaux. 

%\subsection{Contamination}

%\textbf{Objectifs :} Préciser exactement ce que l'on observe dans la contamination (santé humaine, animale et phénomènes de résistance aux substances). \\

%\textbf{Transition :} Nous on s'occupe de la surveillance de concentrations de pesticides dans des milieux environnementaux donc on regarde des données de mesure prélevées par des stations dans le temps et dans l'espace.

\section{Spécificité des mesures de concentrations}

\textbf{Objectifs :} Les problématiques des données de concentrations. 

\subsection{Mesures directes: les prélèvements}
\subsubsection{Hétérogénéité spatio-temporelle}
\subsubsection{Rythme de relève irrégulier}
\subsubsection{Censure à gauche}

\textbf{Transition :} d'autres moyens existent pour avoir une idée de combien de µg/L ont été utilisés. 

\subsection{Mesures indirectes}
\subsubsection{Enquêtes PK}
\subsubsection{BNVD}

\textbf{Transition :} des informations spécifiques au milieu environnemental considéré sont nécessaires parce que celles ci peuvent influencer la dispersion du produit. 

\section{Les données descriptives supplémentaires}

\textbf{Objectif :} données spécifiques aux milieux environnementaux et données météo également \\

%\textbf{Transition :} Combinaison de données de différentes sources d'informations, les experts ont besoin de croiser les informations de ces sources pour travailler. On donne une première approche (naïve) dans la section suivante.

\section{Construire la surveillance de ces données}

\textbf{Objectif :} Exploiter une combinaison de données issues de différentes sources d'informations, c'est un problème complexe sur lequel sortir des diagnostics automatiques ne semble pas être le plus efficace. Besoin donc de connaissance experte (l'humain). Pour pouvoir explorer ces données les experts ont besoin d'outils permettant de croiser les informations de ces sources pour travailler et cela passe par exemple par des outils de visualisation. (s'appuyer sur des références de surveys en visualisation de données) en donner les limites. 

%\textbf{Objectif :} qu'est ce qu'il est possible de faire en représentation (s'appuyer sur des références de surveys en visualisation de données), pourquoi c'est pas très pratique et parler d'application parler des applications interactives (du style Rshiny peut être trouver un exemple d'une pré existante à l'Anses disponible au public ou alors une source qui montre l'intérêt de l'Anses pour ces applications). \\

%\textbf{Transition :} Notre plus-value, nous on souhaite développer une application qui combine un travail de modélisation et de visualisation, dont l'objectif est de trouver de manière automatique des anomalies spatio-temporelle de concentration.    

\chapter{Chapitre Change-point detection}

\textbf{Objectif :} Une fois autorisée, utilisation de substance n'est pas sous contrôle de l'agence, l'utilisation n'est donc pas uniforme sur l'ensemble du territoire et très variable dans le temps (en fonction de la météo et de l'évolution climatique qui elle même dépend de la géographie). L'objectif est de trouver des périodes de temps stable, homogène spatio-temporellement. Pour ce qui est du temporel, on utilise ici de la détection de ruptures pour trouver ce qui esgt stable temporellement. Il ya des surveys qui existent (les citer) on ne couvre ici que ce qui nous a semblé cohérent.

%Aborder la partie modélisation surtout parler de l'aspect temporel, l'irrégularité temporelle plus la censure. On cherche à trouver des segments temporels où les concentrations sont homogènes. Dans la littérature les méthodes de détections de change-points permettent de le faire. Exit les méthode online en donner des références. Exit les méthodes bayésiennes en donner des références.

\section{Modélisation}

\textbf{Objectif :} introduction des fonctions de coût, inférence paramétriques et non paramétriques. Pas de formulation de problème pénalisé dans cette section.
\subsection{Inférence paramétrique}
\subsection{Inférence non-paramétrique}

\section{Recherche de points de ruptures}

\textbf{Objectif :} Exit les méthodes approximatives (binseg, window sliding) ce n'est pas ce que l'on veut (on peut donner des références). On cherche une solution exacte au problème des ruptures la taille des données le permet.

\subsection{Partition optimale}

\textbf{Objectif :} Donner la description plus algo pseudo code. Méthode lente et qui nécessite une nombre de rupture connu. Cependant grâce à différentes méthodes on peut estimer le nombre de rupture qunad il n'est pas connu : 1. on sort toutes les segopt pour K allant de 1 à Kmax puis on utilise une stratégie de pénalisation 2. méthode du coude.  

\subsection{PELT}

\textbf{Objectif :} Sous certaines conditions sur la pénalité , on peut reformuler le problème et utiliser pelt. Donner la description plus algo pseudo code. Avantage : Méthode plus rapide. Inconvénient : opacité de la pénalité, on ne sait pas combien de ruptures on va obtenir pour une valeur donnée de pénalité.

%\textbf{Transition :} 
%\begin{itemize}
%\item Avantage : aucune modélisation du temps n'est faite avec ces deux méthodes, permet de contourner le problème du sampling irrégulier. 
%\item Problème est que avec PELT, pour une valeur de pénalité est associée une segmentation optimale. Quand on utilise la partition optimale, on obtient toutes les segmentations (bien que ce soit lent) pour des nombres de ruptures inférieur à un $K_{max}$ fixé. Avoir plusieurs choix de segmentations est préférable dans une démarche exploratoire. Comment utiliser les avantages de PELT dans ce type de démarche ?
%\end{itemize}

\section{CROPS}


\textbf{Objectif :} Compromis pour atteindre l'exhaustivité de segopt et la rapidité de PELT. CROPS. Pour une démarche exploratoire ce qui semble être le plus cohérent car permet de se sortir du fait que dans PELT la valeur de pen induit le nombre de rupture.  
%\textbf{Objectif :} Donner les méthodes pour obtenir plusieurs segmentations : parler de CROPS. Pour la sélection de modèle, parler de la méthode du coude ?

\section{Détection de ruptures sur des données de pesticides}

\textbf{Objectif :} Un tour de littérature sur les applications pré existantes leurs limites dans notre cadre.

\chapter{Chapitre modèle de détection de rupture dans des données censurées à gauche}

\textbf{Objectif :} Décrire ce que l'on a développé dans le cadre le plus général possible (donc Weibull pas tout de suite). Voir comment on peut insérer la censure à gauche (ou à droite) dans des méthodes de détection de ruptures et voir les difficultés que ça pose.

\section{Effet de la censure}

\textbf{Objectif :} introduction d'un paramètre de régularisation pour les segments complètement censurés. Illustration, loi exponentielle. 

\section{Gestion du multi-paramètre}

\textbf{Objectif :} stratégie détection sur tous les paramètres. et stratégie intermédiaire paramètre global solution d'optimisation alternée (présentation de l'heuristique pour estimer le sigma sans forcément mentionner Weibull). Illustration loi de Weibull. 

\section{Comparaison avec Multrank}

\textbf{Objectif :} expériences sur la taille minimale de segment et de comparaison avec Multrank

\textbf{Remarque:} Newton Raphson passe en annexe ou saute.

%\section{Modèle}

%\subsection{Le modèle de rupture}

%\textbf{Objectif :} Donner la formule du problème pénalisé avec un fonction de coût paramétrique, les hypothèses de convergence des estimateurs et mettre les preuves en annexes.

%\subsection{Adaptation à l'observation des données}

%\textbf{Objectif :} On s'intéresse à des lois ayant des paramètres de dispersion et d'intensité. Donner la modélisation censure à gauche en écrivant la log vraisemblance (dans le cadre général). 

%\section{Procédure d'estimation}

%\subsection{L'estimation du nombre et positions de ruptures}

%\textbf{Objectif :} Cas d'application de PELT que l'on peut utiliser car les conditions sont réunies. Bien mentionner que l'application de Pelt nécessite une taille minimale de segment pour fonctionner. Pour choisir une valeur de pénalité, on utilise CROPS. 

%\subsection{L'estimation des paramètres d'un segment en cas de censure}

%\textbf{Objectif :} Newton-raphson, discussion sur $lambda$ maximum dans le cas d'un segment entièrement censuré et sur l'existence d'un minimum global (la dérivée seconde n'est pas forcément tout le temps positive mais on peut montrer l'existence d'un minimum global dans le cas d'une Weibull par exemple). 

%\section{Expériences de simulation}

%\textbf{Objectif :} Faire des essais pour calibrer notre méthode, on choisit une loi de Weibull. Ensuite la comparer à multrank.

%\subsection{Calibrer Newton-Raphson}

%\textbf{Objectif :}
%\begin{itemize}
%\item Parler de l'importance de l'initialisation et comparer les différentes initialisations possibles.
%\item Après avoir lu vos retours, . Le seuil de précision n'est pas très intéressant.  
%\end{itemize}

%\subsection{Calibrer la taille minimale de segment}

%\textbf{Objectif :} Montrer la capacité de détection d'une rupture avec les Area Under the Curve (faire varier le taux de censure).

%\subsection{Comparaison avec MultRank}

%\textbf{Objectif :} Regarder qui s'en sort le mieux pour trouver le nombre et la position des ruptures dans des signaux simulés.  

\chapter{Application on real data}

\textbf{Objectif :} Montrer les résultats convaincants et leur intérêt sur le prosulfocarb.  

\section{Collectes de données et modèle sous-jacent}

\textbf{Objectif :} Sous quelle forme disposons nous des données et comment allons les exploiter. 

\subsection{Graphe des stations}

\textbf{Objectif :} Définition du graphe des stations.

\subsection{Collecte de données}

\textbf{Objectif :} Description et notations pour les relevés faits par les stations et définition de la série des max journaliers. renvoyer au chapitre 4 pour la modélisation des max.

%\subsection{Modélisation de la série des maximums journaliers}

%\textbf{Objectif :} Poser le modélisation du modèle de ruptures sur la série des maximums journaliers. Préciser que l'on modélise par Weibull ici ? 

\section{Procédure d'estimation}

\textbf{Objectif :} La segmentation de la série temporelle s'obtient en utilisant la procédure de 4. Comment se gère le clustering spatial et les scores d'anomalie ? 

%\subsection{Détection de ruptures dans les maximums journaliers}

%\textbf{Objectif :} En suivant la procédure du chapitre 4, on peut obtenir le nombre de ruptures leur position et les paramètres de chaque segment. Cette partie est surtout pour introduire la méthode d'estimation du paramètre de dispersion sigma. 

\subsection{Clustering spatial}

\textbf{Objectif :} Présenter les deux méthodes de clustering spatiale possible pour obtenir un clustering spatial en présence de composantes non connexes. Les deux méthodes sont la méthode gloutonne (celle utilisée dans l'article), la méthode par programmation dynamique que j'ai mis en place grâce au papier Hébrail (et Rossi) de clustering et segmentation simultané de données fonctionnelles. Etape indépendante de la segmentation temporelle, toutes les stations du graphes sont utilisées. 

\subsection{Détection d'anomalies}

\textbf{Objectif :} Introduire le front de Pareto utilisée sur toutes les données d'un cluster pendant la période temporelle. 

\section{Présentation des données}

\textbf{Objectif :} Expliquer le contexte pourquoi on travaille sur cette substance à cet endroit et sur cette période et faire travail de statistique descriptif. 

\subsection{Choix de la période temporelle et et de la zone d'étude}

\textbf{Objectif :} Justification du contexte applicatif (ventes croissantes, statut de la substance et sa toxicité sur les organismes aquatiques d'où le fait de regarder les eaux de surface). Montrer la série des maximums journaliers.

\subsection{Le graphe des stations associé}

\textbf{Objectif :} Décrire la construction du graphe avec l'utilisation de l'IGN. 

\section{Résultats}

\subsection{Segmentation temporelle}

\textbf{Objectif :} Montrer la segmentation obtenue pour les max journaliers (méthode du coude en annexe). Commenter les résultats.

\subsection{Résultats du clustering du graphe}

\textbf{Objectif :} Montrer les clusters obtenus pour le clustering optimal (méthode du coude en annexe). Commenter les résultats. 

\subsection{Détection de clusters anormaux}

\textbf{Objectif :} Montrer la cartographie des anomalies (tracé des fronts de Pareto en annexe). Commenter les résultats. 

\chapter{Operational tools}

\textbf{Objectif :} faire une démo avec des captures d'écran. Passer les détails style si je clique là il se passe quoi.

\chapter{Conclusion}

\section{Résumé de notre travail}

\textbf{Objectif :} Reprendre la problématique du deuxième chapitre, dire que malgré les caractéristiques des données réelles on a réussi a en extraire des informations importantes. Montrer les limites des résultats (le nombre de données d'un cluster lors de la détection d'anomalies dans un segment temporel notamment)

\section{Axes d'améliorations}

\textbf{Objectif :}
\begin{itemize}
\item les améliorations que l'on peut apporter sur ce que l'on a déjà fait (multivarié, comparaison de positions de ruptures sur des séries temporelles indépendantes, suivi de l'évolution des fronts de pareto des clusters dans le temps)
\item les autres pistes de travail : améliorer et uniformiser les pratiques de collectes de données. Parler de l'optimal design comme piste. 
\end{itemize}

\end{document}